{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse of the textual data\n",
    "\n",
    "Reasearch questions :\n",
    "* What are the main topic of the ads\n",
    "* Whats is the favorite topic of each parti\n",
    "* What are the global sentiment in these ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os, codecs, string, random\n",
    "import numpy as np\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#NLP libraries\n",
    "import spacy, nltk, gensim, sklearn\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "#Vader\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Scikit imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_PATH = '../generated/'\n",
    "DATA_PATH = '../data/'\n",
    "\n",
    "vlaams = ['vlaams_belang', 'nva', 'spa', 'cd&v', 'openvld', 'groen', 'pvda']\n",
    "wallon = ['defi', 'mr', 'ps', 'ecolo', 'ptb', 'cdh']\n",
    "\n",
    "ads = pd.read_csv(GENERATED_PATH+'clean_ads.csv', index_col='id', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads['text'] = ads['text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to separate ads in flamish and ads in french\n",
    "nl_ads = ads[ads.parti.isin(vlaams)]\n",
    "fr_ads = ads[ads.parti.isin(wallon)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lamborellemaxime/anaconda3/envs/ada/lib/python3.7/site-packages/msgpack_numpy.py:184: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "/Users/lamborellemaxime/anaconda3/envs/ada/lib/python3.7/site-packages/msgpack_numpy.py:184: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "/Users/lamborellemaxime/anaconda3/envs/ada/lib/python3.7/site-packages/msgpack_numpy.py:184: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "/Users/lamborellemaxime/anaconda3/envs/ada/lib/python3.7/site-packages/msgpack_numpy.py:184: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "/Users/lamborellemaxime/anaconda3/envs/ada/lib/python3.7/site-packages/msgpack_numpy.py:184: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "/Users/lamborellemaxime/anaconda3/envs/ada/lib/python3.7/site-packages/msgpack_numpy.py:184: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "/Users/lamborellemaxime/anaconda3/envs/ada/lib/python3.7/site-packages/msgpack_numpy.py:184: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "/Users/lamborellemaxime/anaconda3/envs/ada/lib/python3.7/site-packages/msgpack_numpy.py:184: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.nl.stop_words import STOP_WORDS\n",
    "chunks = nl_ads.text.str.lower().to_list()\n",
    "\n",
    "nlp = spacy.load('nl')\n",
    "\n",
    "\n",
    "processed_docs = list()\n",
    "for doc in nlp.pipe(chunks, n_threads=5, batch_size=10):\n",
    "\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    ents = doc.ents  # Named entities\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list and keep only words of length 3 or more.\n",
    "    doc = [token for token in doc if token not in STOP_WORDS and len(token) > 2]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "\n",
    "    processed_docs.append(doc)\n",
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams too\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "# Add bigrams to docs (only ones that appear 15 times or more).\n",
    "bigram = Phrases(docs, min_count=15)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 1556\n",
      "Number of chunks: 2776\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 5\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "#MmCorpus.serialize(\"models/corpus.mm\", corpus)\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of chunks: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "from gensim.models import LdaMulticore\n",
    "params = {'passes': 10, 'random_state': seed}\n",
    "base_models = dict()\n",
    "model = LdaMulticore(corpus=corpus, num_topics=10, id2word=dictionary, workers=6,\n",
    "                passes=params['passes'], random_state=params['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.040*\"nan\" + 0.022*\"kies\" + 0.019*\"leven\" + 0.016*\"mensen\" + 0.016*\"politiek\"'),\n",
       " (1,\n",
       "  '0.025*\"nieuwestrijd\" + 0.023*\"moeten\" + 0.016*\"werken\" + 0.015*\"dag\" + 0.012*\"zekerheid\"'),\n",
       " (2,\n",
       "  '0.020*\"like\" + 0.019*\"mensen\" + 0.018*\"tijd\" + 0.016*\"maggie\" + 0.014*\"block\"'),\n",
       " (3,\n",
       "  '0.020*\"mensen\" + 0.016*\"jij\" + 0.015*\"euro\" + 0.014*\"hetkananders\" + 0.012*\"regering\"'),\n",
       " (4,\n",
       "  '0.068*\"like\" + 0.048*\"steunt\" + 0.040*\"like_steunt\" + 0.028*\"onze\" + 0.026*\"pagina\"'),\n",
       " (5,\n",
       "  '0.024*\"singles\" + 0.017*\"alleenstaanden\" + 0.017*\"bart\" + 0.016*\"singles_alleenstaanden\" + 0.016*\"energie\"'),\n",
       " (6,\n",
       "  '0.024*\"stem\" + 0.023*\"onze\" + 0.020*\"via\" + 0.019*\"vragen\" + 0.018*\"jouw\"'),\n",
       " (7,\n",
       "  '0.046*\"onze\" + 0.016*\"like\" + 0.013*\"bent\" + 0.011*\"terug\" + 0.011*\"steun\"'),\n",
       " (8,\n",
       "  '0.051*\"nieuwestrijd\" + 0.043*\"zekerheid\" + 0.029*\"opnieuw\" + 0.024*\"toekomst\" + 0.021*\"geloven\"'),\n",
       " (9,\n",
       "  '0.024*\"groen\" + 0.022*\"onze\" + 0.017*\"uur\" + 0.014*\"meld\" + 0.012*\"meyrem\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
